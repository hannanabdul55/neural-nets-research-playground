{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import torchvision.datasets as datasets\n",
    "import time\n",
    "import vgg_test as vgg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(flush_secs=10)\n",
    "model = vgg_test.VGG(vgg_test.make_layers(vgg_test.cfg['B'], batch_norm=True, writer=writer), writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=128, shuffle=True,\n",
    "    num_workers=5, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=128, shuffle=False,\n",
    "    num_workers=5, pin_memory=True)\n",
    "\n",
    "# define loss function (criterion) and pptimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 3e-4,\n",
    "                                weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch,writer = SummaryWriter(flush_secs=10)):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "        writer.add_scalar('Loss/Train', losses.val)\n",
    "        writer.add_scalar('Prec@1', top1.val)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 2.879 (2.879)\tData 0.131 (0.131)\tLoss 2.3018 (2.3018)\tPrec@1 10.156 (10.156)\n",
      "Epoch: [0][100/391]\tTime 2.712 (2.646)\tData 0.001 (0.002)\tLoss 1.8017 (2.1109)\tPrec@1 29.688 (17.760)\n",
      "Epoch: [0][200/391]\tTime 2.532 (2.660)\tData 0.001 (0.001)\tLoss 1.7289 (1.9608)\tPrec@1 39.062 (24.036)\n",
      "Epoch: [0][300/391]\tTime 2.547 (2.627)\tData 0.001 (0.001)\tLoss 1.6663 (1.8540)\tPrec@1 38.281 (28.473)\n",
      "Epoch: [1][0/391]\tTime 2.828 (2.828)\tData 0.066 (0.066)\tLoss 1.5148 (1.5148)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [1][100/391]\tTime 2.540 (2.574)\tData 0.001 (0.001)\tLoss 1.4913 (1.4580)\tPrec@1 41.406 (45.498)\n",
      "Epoch: [1][200/391]\tTime 2.588 (2.576)\tData 0.001 (0.001)\tLoss 1.4236 (1.4226)\tPrec@1 44.531 (47.349)\n",
      "Epoch: [1][300/391]\tTime 2.571 (2.579)\tData 0.001 (0.001)\tLoss 1.2627 (1.3835)\tPrec@1 53.125 (49.131)\n",
      "Epoch: [2][0/391]\tTime 2.817 (2.817)\tData 0.067 (0.067)\tLoss 1.3881 (1.3881)\tPrec@1 46.094 (46.094)\n",
      "Epoch: [2][100/391]\tTime 2.602 (2.574)\tData 0.001 (0.001)\tLoss 1.0374 (1.1612)\tPrec@1 62.500 (58.099)\n",
      "Epoch: [2][200/391]\tTime 2.578 (2.571)\tData 0.001 (0.001)\tLoss 1.0099 (1.1354)\tPrec@1 60.938 (59.251)\n",
      "Epoch: [2][300/391]\tTime 2.591 (2.572)\tData 0.001 (0.001)\tLoss 1.0202 (1.1151)\tPrec@1 65.625 (60.299)\n",
      "Epoch: [3][0/391]\tTime 2.877 (2.877)\tData 0.067 (0.067)\tLoss 0.9872 (0.9872)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [3][100/391]\tTime 2.591 (2.584)\tData 0.001 (0.001)\tLoss 1.0212 (0.9791)\tPrec@1 65.625 (65.586)\n",
      "Epoch: [3][200/391]\tTime 2.597 (2.584)\tData 0.001 (0.001)\tLoss 1.3177 (0.9562)\tPrec@1 57.812 (66.406)\n",
      "Epoch: [3][300/391]\tTime 2.568 (2.588)\tData 0.001 (0.001)\tLoss 1.0702 (0.9429)\tPrec@1 60.938 (66.951)\n",
      "Epoch: [4][0/391]\tTime 2.914 (2.914)\tData 0.065 (0.065)\tLoss 0.9885 (0.9885)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [4][100/391]\tTime 2.607 (2.621)\tData 0.001 (0.001)\tLoss 1.0096 (0.8652)\tPrec@1 62.500 (70.011)\n",
      "Epoch: [4][200/391]\tTime 2.616 (2.620)\tData 0.001 (0.001)\tLoss 0.7692 (0.8346)\tPrec@1 70.312 (71.078)\n",
      "Epoch: [4][300/391]\tTime 2.630 (2.624)\tData 0.001 (0.001)\tLoss 0.7830 (0.8276)\tPrec@1 71.094 (71.473)\n",
      "Epoch: [5][0/391]\tTime 2.904 (2.904)\tData 0.065 (0.065)\tLoss 0.7013 (0.7013)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [5][100/391]\tTime 2.774 (2.643)\tData 0.001 (0.001)\tLoss 0.6462 (0.7434)\tPrec@1 75.781 (74.760)\n",
      "Epoch: [5][200/391]\tTime 2.617 (2.641)\tData 0.001 (0.001)\tLoss 0.9245 (0.7454)\tPrec@1 67.188 (74.759)\n",
      "Epoch: [5][300/391]\tTime 2.656 (2.645)\tData 0.001 (0.001)\tLoss 0.7051 (0.7444)\tPrec@1 73.438 (74.795)\n",
      "Epoch: [6][0/391]\tTime 2.947 (2.947)\tData 0.066 (0.066)\tLoss 0.6228 (0.6228)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [6][100/391]\tTime 2.641 (2.665)\tData 0.001 (0.001)\tLoss 0.6071 (0.6920)\tPrec@1 80.469 (76.787)\n",
      "Epoch: [6][200/391]\tTime 2.671 (2.669)\tData 0.001 (0.001)\tLoss 0.5753 (0.6765)\tPrec@1 79.688 (77.274)\n",
      "Epoch: [6][300/391]\tTime 2.673 (2.670)\tData 0.001 (0.001)\tLoss 0.7968 (0.6726)\tPrec@1 76.562 (77.471)\n",
      "Epoch: [7][0/391]\tTime 2.960 (2.960)\tData 0.065 (0.065)\tLoss 0.5526 (0.5526)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [7][100/391]\tTime 2.688 (2.747)\tData 0.001 (0.001)\tLoss 0.7566 (0.6023)\tPrec@1 77.344 (79.633)\n",
      "Epoch: [7][200/391]\tTime 2.697 (2.741)\tData 0.001 (0.001)\tLoss 0.5897 (0.6172)\tPrec@1 75.000 (79.256)\n",
      "Epoch: [7][300/391]\tTime 2.741 (2.740)\tData 0.001 (0.001)\tLoss 0.6414 (0.6159)\tPrec@1 76.562 (79.342)\n",
      "Epoch: [8][0/391]\tTime 3.023 (3.023)\tData 0.065 (0.065)\tLoss 0.5640 (0.5640)\tPrec@1 80.469 (80.469)\n",
      "Epoch: [8][100/391]\tTime 2.760 (2.763)\tData 0.001 (0.001)\tLoss 0.5524 (0.5691)\tPrec@1 80.469 (81.498)\n",
      "Epoch: [8][200/391]\tTime 2.777 (2.770)\tData 0.001 (0.001)\tLoss 0.4851 (0.5682)\tPrec@1 80.469 (81.374)\n",
      "Epoch: [8][300/391]\tTime 2.810 (2.781)\tData 0.001 (0.001)\tLoss 0.4925 (0.5623)\tPrec@1 82.812 (81.600)\n",
      "Epoch: [9][0/391]\tTime 3.111 (3.111)\tData 0.068 (0.068)\tLoss 0.7574 (0.7574)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [9][100/391]\tTime 2.869 (2.926)\tData 0.001 (0.001)\tLoss 0.5628 (0.5248)\tPrec@1 80.469 (82.696)\n",
      "Epoch: [9][200/391]\tTime 2.837 (2.907)\tData 0.001 (0.001)\tLoss 0.5417 (0.5301)\tPrec@1 84.375 (82.373)\n",
      "Epoch: [9][300/391]\tTime 2.935 (2.908)\tData 0.001 (0.001)\tLoss 0.6214 (0.5326)\tPrec@1 78.906 (82.522)\n",
      "Epoch: [10][0/391]\tTime 3.217 (3.217)\tData 0.066 (0.066)\tLoss 0.4714 (0.4714)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [10][100/391]\tTime 2.986 (2.980)\tData 0.001 (0.001)\tLoss 0.4676 (0.5079)\tPrec@1 85.938 (83.106)\n",
      "Epoch: [10][200/391]\tTime 3.004 (2.999)\tData 0.001 (0.001)\tLoss 0.6138 (0.5055)\tPrec@1 82.812 (83.166)\n",
      "Epoch: [10][300/391]\tTime 3.045 (3.014)\tData 0.001 (0.001)\tLoss 0.4569 (0.4970)\tPrec@1 85.156 (83.482)\n",
      "Epoch: [11][0/391]\tTime 3.337 (3.337)\tData 0.065 (0.065)\tLoss 0.4233 (0.4233)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [11][100/391]\tTime 3.166 (3.138)\tData 0.001 (0.001)\tLoss 0.5054 (0.4603)\tPrec@1 82.031 (85.002)\n",
      "Epoch: [11][200/391]\tTime 3.254 (3.157)\tData 0.001 (0.001)\tLoss 0.4348 (0.4632)\tPrec@1 85.938 (84.950)\n",
      "Epoch: [11][300/391]\tTime 3.282 (3.188)\tData 0.001 (0.001)\tLoss 0.3820 (0.4702)\tPrec@1 88.281 (84.661)\n",
      "Epoch: [12][0/391]\tTime 3.718 (3.718)\tData 0.066 (0.066)\tLoss 0.3086 (0.3086)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [12][100/391]\tTime 3.571 (3.482)\tData 0.001 (0.001)\tLoss 0.4981 (0.4351)\tPrec@1 85.938 (85.883)\n",
      "Epoch: [12][200/391]\tTime 3.592 (3.539)\tData 0.001 (0.001)\tLoss 0.3858 (0.4446)\tPrec@1 89.062 (85.576)\n",
      "Epoch: [12][300/391]\tTime 3.703 (3.586)\tData 0.001 (0.001)\tLoss 0.3502 (0.4440)\tPrec@1 85.938 (85.507)\n",
      "Epoch: [13][0/391]\tTime 4.066 (4.066)\tData 0.067 (0.067)\tLoss 0.4642 (0.4642)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [13][100/391]\tTime 3.948 (3.880)\tData 0.001 (0.001)\tLoss 0.4123 (0.4267)\tPrec@1 85.156 (85.883)\n",
      "Epoch: [13][200/391]\tTime 4.049 (3.938)\tData 0.001 (0.001)\tLoss 0.4883 (0.4217)\tPrec@1 83.594 (85.992)\n",
      "Epoch: [13][300/391]\tTime 4.238 (4.015)\tData 0.001 (0.001)\tLoss 0.3948 (0.4204)\tPrec@1 86.719 (86.026)\n",
      "Epoch: [14][0/391]\tTime 4.612 (4.612)\tData 0.067 (0.067)\tLoss 0.3891 (0.3891)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [14][100/391]\tTime 4.425 (4.389)\tData 0.001 (0.001)\tLoss 0.2760 (0.3744)\tPrec@1 89.062 (87.995)\n",
      "Epoch: [14][200/391]\tTime 4.457 (4.441)\tData 0.001 (0.001)\tLoss 0.4179 (0.3956)\tPrec@1 85.156 (87.228)\n",
      "Epoch: [14][300/391]\tTime 4.747 (4.497)\tData 0.001 (0.001)\tLoss 0.2518 (0.3970)\tPrec@1 92.188 (87.199)\n",
      "Epoch: [15][0/391]\tTime 5.000 (5.000)\tData 0.066 (0.066)\tLoss 0.3184 (0.3184)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [15][100/391]\tTime 4.916 (4.859)\tData 0.001 (0.001)\tLoss 0.2927 (0.3872)\tPrec@1 90.625 (87.330)\n",
      "Epoch: [15][200/391]\tTime 5.221 (4.908)\tData 0.001 (0.001)\tLoss 0.4641 (0.3805)\tPrec@1 82.812 (87.605)\n",
      "Epoch: [15][300/391]\tTime 5.031 (4.947)\tData 0.001 (0.001)\tLoss 0.3501 (0.3797)\tPrec@1 89.844 (87.604)\n",
      "Epoch: [16][0/391]\tTime 5.089 (5.089)\tData 0.070 (0.070)\tLoss 0.3835 (0.3835)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [16][100/391]\tTime 4.858 (4.917)\tData 0.001 (0.001)\tLoss 0.2598 (0.3622)\tPrec@1 92.188 (88.049)\n",
      "Epoch: [16][200/391]\tTime 4.894 (4.906)\tData 0.001 (0.001)\tLoss 0.4058 (0.3672)\tPrec@1 87.500 (88.021)\n",
      "Epoch: [16][300/391]\tTime 4.891 (4.916)\tData 0.001 (0.001)\tLoss 0.3997 (0.3695)\tPrec@1 85.938 (88.001)\n",
      "Epoch: [17][0/391]\tTime 5.324 (5.324)\tData 0.072 (0.072)\tLoss 0.3776 (0.3776)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [17][100/391]\tTime 5.098 (5.114)\tData 0.001 (0.001)\tLoss 0.3667 (0.3400)\tPrec@1 82.812 (88.575)\n",
      "Epoch: [17][200/391]\tTime 5.140 (5.142)\tData 0.001 (0.001)\tLoss 0.3226 (0.3465)\tPrec@1 87.500 (88.534)\n",
      "Epoch: [17][300/391]\tTime 5.271 (5.160)\tData 0.001 (0.001)\tLoss 0.3201 (0.3481)\tPrec@1 86.719 (88.554)\n",
      "Epoch: [18][0/391]\tTime 5.560 (5.560)\tData 0.071 (0.071)\tLoss 0.3710 (0.3710)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [18][100/391]\tTime 5.293 (5.315)\tData 0.001 (0.001)\tLoss 0.3126 (0.3270)\tPrec@1 90.625 (89.225)\n",
      "Epoch: [18][200/391]\tTime 5.572 (5.337)\tData 0.001 (0.001)\tLoss 0.3655 (0.3299)\tPrec@1 86.719 (89.280)\n",
      "Epoch: [18][300/391]\tTime 5.446 (5.345)\tData 0.001 (0.001)\tLoss 0.3678 (0.3296)\tPrec@1 90.625 (89.270)\n",
      "Epoch: [19][0/391]\tTime 5.736 (5.736)\tData 0.073 (0.073)\tLoss 0.3708 (0.3708)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [19][100/391]\tTime 5.498 (5.599)\tData 0.001 (0.001)\tLoss 0.2443 (0.3157)\tPrec@1 92.969 (89.735)\n",
      "Epoch: [19][200/391]\tTime 5.675 (5.714)\tData 0.001 (0.001)\tLoss 0.3860 (0.3140)\tPrec@1 89.844 (89.739)\n",
      "Epoch: [19][300/391]\tTime 5.644 (5.670)\tData 0.001 (0.001)\tLoss 0.3554 (0.3174)\tPrec@1 86.719 (89.665)\n"
     ]
    }
   ],
   "source": [
    "train_epochs = 20\n",
    "for i in range(train_epochs):\n",
    "    train(train_loader, model, criterion, optimizer, i, writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/79]\tTime 3.003 (3.003)\tLoss 0.2793 (0.2793)\tPrec@1 90.625 (90.625)\n",
      " * Prec@1 86.870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.87"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model_scale.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wo = torchvision.models.vgg11()\n",
    "optimizer_wo = torch.optim.Adam(model_wo.parameters(), 3e-4,\n",
    "                                weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, model_wo, criterion, optimizer_wo, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('validation_error: ' , get_model_error(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
