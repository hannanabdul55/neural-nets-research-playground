{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch module to test scaling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "# ! pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from scale_layer import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(model_p, sparsity=1e-4):\n",
    "    params = torch.cat([param.view(-1) for param in list(model_p.parameters())])\n",
    "    params_n = params.detach().numpy()\n",
    "    N = params_n.size\n",
    "    stats = {}\n",
    "    print(\"Mean\")\n",
    "    stats['mean'] = np.mean(np.abs(params_n))\n",
    "    print(stats['mean'])\n",
    "    print(\"standard deviation\")\n",
    "    stats['std'] = np.sqrt(np.var(np.abs(params_n)))\n",
    "    print(stats['std'])\n",
    "    print(\"Sparsity\")\n",
    "    print(np.sum(np.abs(params_n)<sparsity)/N)\n",
    "\n",
    "def get_model_error(model_t):\n",
    "    tot_num = 0.0\n",
    "    num_wrong = 0.0\n",
    "    dataiter = iter(testloader)\n",
    "    for images,labels in dataiter:\n",
    "        outputs = model_t(images)\n",
    "        for image, label, i in zip(images, labels, range(len(images))):\n",
    "    #         imshow(image)\n",
    "    #         print(model(image).detach().numpy())\n",
    "    #         expected_class = classes[outputs[i].detach().numpy().argmax()]\n",
    "            expected_class_i = int(outputs[i].detach().numpy().argmax())\n",
    "    #         print(\"Predicted class: %s\" % expected_class)\n",
    "    #         print(\"expected: \" + str(int(label)))\n",
    "            tot_num+=1\n",
    "            if int(label) != expected_class_i:\n",
    "                num_wrong+=1\n",
    "    error = num_wrong/tot_num\n",
    "    print(\"test error: \" + str(error))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()#,transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "# print(torchvision.datasets.__dict__)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(flush_secs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "model_new_t = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Conv2d(16, 32, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(5408, 1152),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1152, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "model_new = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Conv2d(16, 32, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(5408, 1152),\n",
    "    ScaleLayer(writer=writer),\n",
    "    nn.Linear(1152, 512),\n",
    "    ScaleLayer(),\n",
    "    nn.Linear(512, 128),\n",
    "    ScaleLayer(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "## Define the loss function to be used\n",
    "def criterion_p(op, y, model_p):\n",
    "    params = list(model_p.parameters())\n",
    "    return ce_loss(op, y) + (1/len(params))*(torch.norm(torch.cat([param.view(-1) for param in params]))) #+ torch.sum((1/len(params))*torch.exp(torch.abs(torch.cat([param.view(-1) for param in params]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 108.735\n",
      "[1,   600] loss: 95.707\n",
      "loss for epoch 1: 19.984\n",
      "[2,   300] loss: 81.667\n",
      "[2,   600] loss: 80.929\n",
      "loss for epoch 2: 23.758\n",
      "[3,   300] loss: 80.859\n",
      "[3,   600] loss: 74.632\n",
      "loss for epoch 3: 16.350\n",
      "[4,   300] loss: 82.916\n",
      "[4,   600] loss: 86.461\n",
      "loss for epoch 4: 16.582\n",
      "[5,   300] loss: 79.546\n",
      "[5,   600] loss: 79.397\n",
      "loss for epoch 5: 20.884\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "optimizer_new = optim.SGD(model_new.parameters(), lr=1e-4, momentum=0.9, weight_decay = 1/len(list(model_new.parameters())))\n",
    "# optimizer_old = optim.SGD(model_new.parameters(), lr=0.01, momentum=0.9, weight_decay = 1/len(list(model_new.parameters())))\n",
    "optimizer_adam_new = optim.Adam(list(model_new.parameters()), lr=3e-4)\n",
    "# writer.add_graph(model_new, trainset)\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_adam_new.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_new(inputs)\n",
    "        loss = criterion_p(outputs, labels, model_new)\n",
    "        loss.backward()\n",
    "        optimizer_adam_new.step()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item())\n",
    "            \n",
    "        # print statistics\n",
    "#         running_loss += loss.item()\n",
    "        if i%10 == 9:\n",
    "            loss_history.append(loss.item())\n",
    "        if i % 300 ==299:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] running_loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss.item()))\n",
    "#             running_loss = 0.0\n",
    "    writer.close()\n",
    "    print('loss for epoch %d: %.3f' % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 0.3828\n",
      "Mean\n",
      "0.014723567\n",
      "standard deviation\n",
      "0.012330709\n",
      "Sparsity\n",
      "0.004627226684205777\n"
     ]
    }
   ],
   "source": [
    "err_n = get_model_error(model_new)\n",
    "print_stats(model_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Conv2d(16, 32, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(5408, 1152),\n",
    "    nn.Tanh(),\n",
    "    nn.BatchNorm1d(1152),\n",
    "    nn.Linear(1152, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Linear(128, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_bn = []\n",
    "writer_bn = SummaryWriter(log_dir='runs/bn_stats', flush_secs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 80.156\n",
      "[1,   600] loss: 86.769\n",
      "loss for epoch 1: 22.074\n",
      "[2,   300] loss: 75.361\n",
      "[2,   600] loss: 75.502\n",
      "loss for epoch 2: 27.813\n",
      "[3,   300] loss: 64.358\n",
      "[3,   600] loss: 67.213\n",
      "loss for epoch 3: 19.942\n",
      "[4,   300] loss: 73.064\n",
      "[4,   600] loss: 58.656\n",
      "loss for epoch 4: 18.490\n",
      "[5,   300] loss: 58.896\n",
      "[5,   600] loss: 45.755\n",
      "loss for epoch 5: 29.817\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "optimizer_new = optim.SGD(model_bn.parameters(), lr=1e-4, momentum=0.9, weight_decay = 1/len(list(model_bn.parameters())))\n",
    "# optimizer_old = optim.SGD(model_new.parameters(), lr=0.01, momentum=0.9, weight_decay = 1/len(list(model_new.parameters())))\n",
    "optimizer_adam_new = optim.Adam(list(model_bn.parameters()), lr=3e-4)\n",
    "# writer.add_graph(model_new, trainset)\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_adam_new.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_bn(inputs)\n",
    "        loss = criterion_p(outputs, labels, model_bn)\n",
    "        loss.backward()\n",
    "        optimizer_adam_new.step()\n",
    "        if writer is not None:\n",
    "            writer_bn.add_scalar(\"Loss/Train\", loss.item())\n",
    "            \n",
    "        # print statistics\n",
    "#         running_loss += loss.item()\n",
    "        if i%10 == 9:\n",
    "            loss_history_bn.append(loss.item())\n",
    "        if i % 300 ==299:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] running_loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss.item()))\n",
    "#             running_loss = 0.0\n",
    "    writer_bn.close()\n",
    "    print('loss for epoch %d: %.3f' % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wo_bn = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Conv2d(16, 32, 3),\n",
    "    nn.FractionalMaxPool2d(2, output_ratio=(1/np.sqrt(2))),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(5408, 1152),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1152, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_wo_bn = []\n",
    "writer_wo_bn = SummaryWriter(log_dir='runs/wo_bn_stats', flush_secs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "optimizer_new = optim.SGD(model_wo_bn.parameters(), lr=1e-4, momentum=0.9, weight_decay = 1/len(list(model_wo_bn.parameters())))\n",
    "# optimizer_old = optim.SGD(model_new.parameters(), lr=0.01, momentum=0.9, weight_decay = 1/len(list(model_new.parameters())))\n",
    "optimizer_adam_new = optim.Adam(list(model_wo_bn.parameters()), lr=3e-4)\n",
    "# writer.add_graph(model_new, trainset)\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_adam_new.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_wo_bn(inputs)\n",
    "        loss = criterion_p(outputs, labels, model_wo_bn)\n",
    "        loss.backward()\n",
    "        optimizer_adam_new.step()\n",
    "        if writer_wo_bn is not None:\n",
    "            writer_wo_bn.add_scalar(\"Loss/Train\", loss.item())\n",
    "            \n",
    "        # print statistics\n",
    "#         running_loss += loss.item()\n",
    "        if i%10 == 9:\n",
    "            loss_history_wo_bn.append(loss.item())\n",
    "        if i % 300 ==299:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] running_loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss.item()))\n",
    "#             running_loss = 0.0\n",
    "    print('loss for epoch %d: %.3f' % (epoch+1, loss.item()))\n",
    "writer_wo_bn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
