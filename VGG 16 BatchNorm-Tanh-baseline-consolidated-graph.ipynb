{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import torchvision.datasets as datasets\n",
    "import time\n",
    "import vgg as vgg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg_test.VGG(vgg_test.make_layers(vgg_test.cfg['B'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=128, shuffle=True,\n",
    "    num_workers=5, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=128, shuffle=False,\n",
    "    num_workers=5, pin_memory=True)\n",
    "\n",
    "# define loss function (criterion) and pptimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 3e-4,\n",
    "                                weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, writer=None):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "        writer.add_scalar('Loss/Train', losses.val)\n",
    "        writer.add_scalar('Prec@1', top1.val)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 3.300 (3.300)\tData 0.139 (0.139)\tLoss 2.3176 (2.3176)\tPrec@1 14.062 (14.062)\n",
      "Epoch: [0][100/391]\tTime 2.916 (3.042)\tData 0.001 (0.002)\tLoss 1.6730 (1.8745)\tPrec@1 36.719 (27.235)\n",
      "Epoch: [0][200/391]\tTime 2.904 (2.983)\tData 0.001 (0.001)\tLoss 1.4101 (1.6934)\tPrec@1 44.531 (35.533)\n",
      "Epoch: [0][300/391]\tTime 2.902 (2.974)\tData 0.001 (0.001)\tLoss 1.2994 (1.5640)\tPrec@1 52.344 (41.256)\n",
      "Epoch: [1][0/391]\tTime 3.204 (3.204)\tData 0.076 (0.076)\tLoss 1.1031 (1.1031)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [1][100/391]\tTime 2.882 (2.914)\tData 0.001 (0.002)\tLoss 1.0164 (1.1234)\tPrec@1 60.156 (60.195)\n",
      "Epoch: [1][200/391]\tTime 2.891 (2.901)\tData 0.001 (0.001)\tLoss 0.9828 (1.0633)\tPrec@1 64.844 (62.240)\n",
      "Epoch: [1][300/391]\tTime 2.886 (2.893)\tData 0.001 (0.001)\tLoss 0.7990 (1.0351)\tPrec@1 75.781 (63.408)\n",
      "Epoch: [2][0/391]\tTime 3.179 (3.179)\tData 0.077 (0.077)\tLoss 0.8211 (0.8211)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [2][100/391]\tTime 2.874 (2.884)\tData 0.001 (0.002)\tLoss 0.9110 (0.8684)\tPrec@1 65.625 (69.763)\n",
      "Epoch: [2][200/391]\tTime 2.991 (3.022)\tData 0.001 (0.001)\tLoss 0.7744 (0.8570)\tPrec@1 71.094 (70.324)\n",
      "Epoch: [2][300/391]\tTime 3.492 (3.069)\tData 0.001 (0.001)\tLoss 0.7954 (0.8343)\tPrec@1 73.438 (71.288)\n",
      "Epoch: [3][0/391]\tTime 3.347 (3.347)\tData 0.077 (0.077)\tLoss 0.8771 (0.8771)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [3][100/391]\tTime 4.083 (3.451)\tData 0.001 (0.002)\tLoss 0.6684 (0.7543)\tPrec@1 75.000 (74.799)\n",
      "Epoch: [3][200/391]\tTime 3.736 (3.526)\tData 0.001 (0.001)\tLoss 0.7605 (0.7451)\tPrec@1 77.344 (75.194)\n",
      "Epoch: [3][300/391]\tTime 2.860 (3.418)\tData 0.001 (0.001)\tLoss 0.7229 (0.7296)\tPrec@1 70.312 (75.633)\n",
      "Epoch: [4][0/391]\tTime 3.182 (3.182)\tData 0.076 (0.076)\tLoss 0.6876 (0.6876)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [4][100/391]\tTime 2.882 (2.885)\tData 0.001 (0.002)\tLoss 0.6754 (0.6564)\tPrec@1 78.906 (78.024)\n",
      "Epoch: [4][200/391]\tTime 2.905 (2.888)\tData 0.001 (0.001)\tLoss 0.7472 (0.6617)\tPrec@1 75.000 (77.981)\n",
      "Epoch: [4][300/391]\tTime 3.178 (2.912)\tData 0.001 (0.001)\tLoss 0.6505 (0.6557)\tPrec@1 76.562 (78.107)\n",
      "Epoch: [5][0/391]\tTime 4.036 (4.036)\tData 0.087 (0.087)\tLoss 0.6134 (0.6134)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [5][100/391]\tTime 3.338 (3.385)\tData 0.001 (0.002)\tLoss 0.5060 (0.5873)\tPrec@1 85.938 (80.198)\n",
      "Epoch: [5][200/391]\tTime 3.403 (3.324)\tData 0.001 (0.001)\tLoss 0.8667 (0.5914)\tPrec@1 71.094 (80.138)\n",
      "Epoch: [5][300/391]\tTime 3.137 (3.272)\tData 0.001 (0.001)\tLoss 0.5213 (0.5919)\tPrec@1 79.688 (80.220)\n",
      "Epoch: [6][0/391]\tTime 3.504 (3.504)\tData 0.095 (0.095)\tLoss 0.5243 (0.5243)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [6][100/391]\tTime 3.304 (3.113)\tData 0.001 (0.002)\tLoss 0.4755 (0.5498)\tPrec@1 84.375 (81.668)\n",
      "Epoch: [6][200/391]\tTime 3.278 (3.208)\tData 0.001 (0.001)\tLoss 0.6530 (0.5531)\tPrec@1 77.344 (81.646)\n",
      "Epoch: [6][300/391]\tTime 3.192 (3.217)\tData 0.001 (0.001)\tLoss 0.4204 (0.5598)\tPrec@1 85.938 (81.439)\n",
      "Epoch: [7][0/391]\tTime 3.519 (3.519)\tData 0.077 (0.077)\tLoss 0.3446 (0.3446)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [7][100/391]\tTime 3.148 (3.183)\tData 0.001 (0.001)\tLoss 0.3927 (0.5134)\tPrec@1 85.938 (82.998)\n",
      "Epoch: [7][200/391]\tTime 3.275 (3.222)\tData 0.001 (0.001)\tLoss 0.4709 (0.5221)\tPrec@1 83.594 (82.746)\n",
      "Epoch: [7][300/391]\tTime 3.081 (3.240)\tData 0.001 (0.001)\tLoss 0.6447 (0.5226)\tPrec@1 83.594 (82.662)\n",
      "Epoch: [8][0/391]\tTime 3.500 (3.500)\tData 0.078 (0.078)\tLoss 0.5188 (0.5188)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [8][100/391]\tTime 3.091 (3.110)\tData 0.001 (0.002)\tLoss 0.3868 (0.4926)\tPrec@1 86.719 (83.942)\n",
      "Epoch: [8][200/391]\tTime 3.151 (3.117)\tData 0.001 (0.001)\tLoss 0.4565 (0.4881)\tPrec@1 87.500 (83.912)\n",
      "Epoch: [8][300/391]\tTime 2.993 (3.124)\tData 0.001 (0.001)\tLoss 0.5561 (0.4943)\tPrec@1 80.469 (83.711)\n",
      "Epoch: [9][0/391]\tTime 3.260 (3.260)\tData 0.075 (0.075)\tLoss 0.5039 (0.5039)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [9][100/391]\tTime 2.926 (2.955)\tData 0.001 (0.001)\tLoss 0.4584 (0.4637)\tPrec@1 81.250 (84.777)\n",
      "Epoch: [9][200/391]\tTime 3.328 (3.037)\tData 0.001 (0.001)\tLoss 0.4325 (0.4621)\tPrec@1 84.375 (84.659)\n",
      "Epoch: [9][300/391]\tTime 3.132 (3.078)\tData 0.001 (0.001)\tLoss 0.5010 (0.4650)\tPrec@1 82.031 (84.588)\n",
      "Epoch: [10][0/391]\tTime 3.209 (3.209)\tData 0.081 (0.081)\tLoss 0.4139 (0.4139)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [10][100/391]\tTime 3.116 (3.257)\tData 0.001 (0.002)\tLoss 0.3272 (0.4426)\tPrec@1 87.500 (85.458)\n",
      "Epoch: [10][200/391]\tTime 2.945 (3.171)\tData 0.001 (0.001)\tLoss 0.4517 (0.4409)\tPrec@1 85.938 (85.514)\n",
      "Epoch: [10][300/391]\tTime 3.350 (3.096)\tData 0.001 (0.001)\tLoss 0.4414 (0.4435)\tPrec@1 89.062 (85.470)\n",
      "Epoch: [11][0/391]\tTime 3.210 (3.210)\tData 0.118 (0.118)\tLoss 0.4264 (0.4264)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [11][100/391]\tTime 2.869 (2.924)\tData 0.001 (0.002)\tLoss 0.3300 (0.4220)\tPrec@1 90.625 (86.162)\n",
      "Epoch: [11][200/391]\tTime 2.895 (2.919)\tData 0.001 (0.001)\tLoss 0.3964 (0.4190)\tPrec@1 88.281 (86.159)\n",
      "Epoch: [11][300/391]\tTime 2.902 (2.917)\tData 0.001 (0.001)\tLoss 0.5097 (0.4201)\tPrec@1 80.469 (86.252)\n",
      "Epoch: [12][0/391]\tTime 3.147 (3.147)\tData 0.065 (0.065)\tLoss 0.4299 (0.4299)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [12][100/391]\tTime 3.503 (2.954)\tData 0.001 (0.001)\tLoss 0.4806 (0.3989)\tPrec@1 85.938 (86.757)\n",
      "Epoch: [12][200/391]\tTime 3.314 (3.118)\tData 0.001 (0.001)\tLoss 0.3956 (0.3974)\tPrec@1 88.281 (86.925)\n",
      "Epoch: [12][300/391]\tTime 3.278 (3.175)\tData 0.001 (0.001)\tLoss 0.4347 (0.3977)\tPrec@1 83.594 (86.895)\n",
      "Epoch: [13][0/391]\tTime 3.577 (3.577)\tData 0.071 (0.071)\tLoss 0.4169 (0.4169)\tPrec@1 82.031 (82.031)\n",
      "Epoch: [13][100/391]\tTime 3.305 (3.274)\tData 0.001 (0.001)\tLoss 0.3105 (0.3821)\tPrec@1 90.625 (87.446)\n",
      "Epoch: [13][200/391]\tTime 3.267 (3.282)\tData 0.001 (0.001)\tLoss 0.3241 (0.3820)\tPrec@1 88.281 (87.554)\n",
      "Epoch: [13][300/391]\tTime 2.992 (3.235)\tData 0.001 (0.001)\tLoss 0.2890 (0.3833)\tPrec@1 91.406 (87.461)\n",
      "Epoch: [14][0/391]\tTime 3.743 (3.743)\tData 0.091 (0.091)\tLoss 0.3294 (0.3294)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [14][100/391]\tTime 2.968 (3.003)\tData 0.001 (0.002)\tLoss 0.3258 (0.3678)\tPrec@1 89.844 (88.049)\n",
      "Epoch: [14][200/391]\tTime 3.331 (3.088)\tData 0.001 (0.001)\tLoss 0.3383 (0.3698)\tPrec@1 88.281 (87.885)\n",
      "Epoch: [14][300/391]\tTime 3.479 (3.213)\tData 0.001 (0.001)\tLoss 0.3737 (0.3693)\tPrec@1 89.062 (87.933)\n",
      "Epoch: [15][0/391]\tTime 3.747 (3.747)\tData 0.072 (0.072)\tLoss 0.2182 (0.2182)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [15][100/391]\tTime 3.013 (3.374)\tData 0.001 (0.001)\tLoss 0.4052 (0.3468)\tPrec@1 85.938 (88.769)\n",
      "Epoch: [15][200/391]\tTime 2.994 (3.321)\tData 0.001 (0.001)\tLoss 0.3816 (0.3560)\tPrec@1 86.719 (88.511)\n",
      "Epoch: [15][300/391]\tTime 3.375 (3.304)\tData 0.001 (0.001)\tLoss 0.3817 (0.3582)\tPrec@1 86.719 (88.468)\n",
      "Epoch: [16][0/391]\tTime 3.677 (3.677)\tData 0.070 (0.070)\tLoss 0.4012 (0.4012)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [16][100/391]\tTime 3.361 (3.374)\tData 0.001 (0.001)\tLoss 0.4432 (0.3426)\tPrec@1 86.719 (88.683)\n",
      "Epoch: [16][200/391]\tTime 3.497 (3.376)\tData 0.001 (0.001)\tLoss 0.3127 (0.3449)\tPrec@1 87.500 (88.604)\n",
      "Epoch: [16][300/391]\tTime 2.978 (3.340)\tData 0.001 (0.001)\tLoss 0.5411 (0.3467)\tPrec@1 80.469 (88.536)\n",
      "Epoch: [17][0/391]\tTime 3.235 (3.235)\tData 0.071 (0.071)\tLoss 0.2760 (0.2760)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [17][100/391]\tTime 2.989 (2.986)\tData 0.001 (0.001)\tLoss 0.2603 (0.3250)\tPrec@1 91.406 (89.442)\n",
      "Epoch: [17][200/391]\tTime 3.005 (2.992)\tData 0.001 (0.001)\tLoss 0.4091 (0.3357)\tPrec@1 86.719 (89.012)\n",
      "Epoch: [17][300/391]\tTime 2.987 (2.993)\tData 0.001 (0.001)\tLoss 0.3833 (0.3359)\tPrec@1 86.719 (89.101)\n",
      "Epoch: [18][0/391]\tTime 3.256 (3.256)\tData 0.077 (0.077)\tLoss 0.2917 (0.2917)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [18][100/391]\tTime 2.981 (3.100)\tData 0.001 (0.002)\tLoss 0.3090 (0.3140)\tPrec@1 90.625 (89.542)\n",
      "Epoch: [18][200/391]\tTime 3.054 (3.051)\tData 0.001 (0.001)\tLoss 0.2347 (0.3133)\tPrec@1 92.188 (89.630)\n",
      "Epoch: [18][300/391]\tTime 2.974 (3.040)\tData 0.001 (0.001)\tLoss 0.2154 (0.3140)\tPrec@1 92.969 (89.667)\n",
      "Epoch: [19][0/391]\tTime 3.353 (3.353)\tData 0.077 (0.077)\tLoss 0.1418 (0.1418)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [19][100/391]\tTime 2.989 (3.005)\tData 0.001 (0.002)\tLoss 0.3588 (0.3023)\tPrec@1 92.188 (90.169)\n",
      "Epoch: [19][200/391]\tTime 2.971 (2.994)\tData 0.001 (0.001)\tLoss 0.3483 (0.3144)\tPrec@1 87.500 (89.688)\n",
      "Epoch: [19][300/391]\tTime 2.995 (2.991)\tData 0.001 (0.001)\tLoss 0.3235 (0.3146)\tPrec@1 86.719 (89.623)\n"
     ]
    }
   ],
   "source": [
    "train_epochs = 20\n",
    "writer = SummaryWriter('batchnorm_base', flush_secs=10)\n",
    "for i in range(train_epochs):\n",
    "    train(train_loader, model, criterion, optimizer, i, writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/79]\tTime 0.961 (0.961)\tLoss 0.3031 (0.3031)\tPrec@1 92.969 (92.969)\n",
      " * Prec@1 87.810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.81"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model_base.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wo = torchvision.models.vgg11()\n",
    "optimizer_wo = torch.optim.Adam(model_wo.parameters(), 3e-4,\n",
    "                                weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10000][0/391]\tTime 5.464 (5.464)\tData 0.072 (0.072)\tLoss 1.1889 (1.1889)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [10000][50/391]\tTime 3.341 (3.419)\tData 0.001 (0.002)\tLoss 1.1646 (1.2292)\tPrec@1 54.688 (56.020)\n",
      "Epoch: [10000][100/391]\tTime 3.523 (3.383)\tData 0.001 (0.001)\tLoss 1.1371 (1.2004)\tPrec@1 57.812 (57.356)\n",
      "Epoch: [10000][150/391]\tTime 3.304 (3.403)\tData 0.001 (0.001)\tLoss 0.9491 (1.1710)\tPrec@1 60.938 (58.371)\n",
      "Epoch: [10000][200/391]\tTime 3.264 (3.386)\tData 0.001 (0.001)\tLoss 1.1725 (1.1639)\tPrec@1 57.031 (58.660)\n",
      "Epoch: [10000][250/391]\tTime 3.291 (3.365)\tData 0.001 (0.001)\tLoss 1.0466 (1.1535)\tPrec@1 58.594 (59.008)\n",
      "Epoch: [10000][300/391]\tTime 3.266 (3.359)\tData 0.001 (0.001)\tLoss 1.1317 (1.1394)\tPrec@1 60.156 (59.406)\n",
      "Epoch: [10000][350/391]\tTime 3.211 (3.353)\tData 0.001 (0.001)\tLoss 1.1335 (1.1278)\tPrec@1 56.250 (59.894)\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, model_wo, criterion, optimizer_wo, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 0.446\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-185059696e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation_error: {.3f} '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mget_model_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "print('validation_error: ' , get_model_error(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
